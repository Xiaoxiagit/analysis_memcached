外存介绍文档
=============================
本文将开始介绍`memecached`的外存部分，从如下几个方面进行介绍:

1. 涉及的文档以及每个文档的含义。
2. 官方对外存的介绍。

## 1. 涉及的源文件
外存涉及的源文件主要有以下几个:

| 文件名称 | 文件作用 | 备注 | 其他 |
| ------- | ------ | ---- | ---- |
|`extstore.h/c` | 此文件主要用于定义管理外存的结构体 | 负责管理外存 | 暂无 |
|`storage.h/c` | 此文件主要用于定义外存写出和合并的两个线程 | 负责外存写出和合并 | 暂无 |
|`slab_automove_extstore.h/c` | 此文件用于定义在`slab_resign`线程中使用的外存分配的数据结构以及函数 | 外存中各`slab`重新分配的管理 | 本文档不做介绍 |

外存的方方面面基本就是依靠这三个文档进行管理和使用，后续的文档将详细介绍`extstore.h/c`和`storage.h/c`这四个文件中的内容。对于`slab_automove_extstore.h/c`文件，读者自行结合前面文档中描述的`slab_reassigned`线程进行阅读。

## 2. 官方介绍
官方对于外存的介绍有较为详细的介绍([官方文档](https://github.com/memcached/memcached.wiki.git))，本文也将大量采用官方文档中的描述进行介绍。本小节也算是对前面几个文档的一个总结。

外存对外提供的`settings`中设置参数主要有以下几个:

| 参数名 | 参数含义 | 参数默认取值 | 备注 |
|-------|---------|------------|-----|
|`ext_page_size`| 外存每页的大小 | 64M | 暂无 |
|`ext_wbuf_size`| 每次写出大小(写出缓冲区大小) | 8M | 为了提高写出效率，不能调小 |
|`ext_threads`| 读取/写出外存线程个数 | 1 | 为了降低读取延迟，可以适当调大 |
|`ext_item_size`| 写出内存的`item`的最小长度 | 512b | 不要调整过小，将会减低外存效率 |
|`ext_low_ttl`| 最小过期时间 | 0 | 设置合理值，有利于外存`page`的回收 |
|`ext_max_frag`|合并调整标识 | 0.8 | 当页面空闲度高于此值时，进行页面合并 |
|`ext_drop_unread`|合并调整标识 | false | 在页面合并过程中，是否丢弃长时间没有读取的`item` |
|`ext_compact_under`|合并调整设置| 总页面数/4 | 当空闲页面比例少于此值时，进行页面合并 |
|`ext_drop_under`|合并调整设置| 总页面数/4 | 当`ext_drop_unread`为`true`时，当空闲页面比例小于此值时，页面合并过程中将丢弃`COLD item`|
|`ext_recache_rate`|`item`返回`RAM`最小访问次数| 2000 | 当`item`1分钟被访问次数超过此值，将从外存返回到`RAM`中 |

### 2.1 高层次的描述
原文描述如下:

A single file is split into N logical pages of Y size (64M default).Write buffers (8M+) are used to store objects later flushed into pages.

IO threads are used to asynchronously flush write buffers and read items back.

Items are left in the hash table along with their keys, with small (24 byte as of this writing) headers detailing where data lives in flash.

The storage engine recycles pages by changing their version number.Item headers remember both the page ID and version number at the time the object was flushed. An attempted read with the wrong version will result in a miss.

The system is most useful if you have a breakdown of different item sizes: pools with a mix of small and large, or if you can break off a small pool of
machines with flash devices to store much larger items. Freeing up more memory on your small item pools can dramatically improve hitrates, or simply reduce costs overall.

从上述的描述可知，一个外存文件被分为多个`page`。`page`的每次读写的大小至少是一个`buffer`(8M)的大小。`IO`线程异步的将`buffer`写出到外存或者从外存读取`item`。外存对应的`item`链接`Hash`链表时，使用`key`对应的`Hash`值，其内容主要包括`page id/page version/offset`这三个值。外存通过改变`page version`来进行`page`回收，当`item`中的`page version`跟`page`(页面)中的`version`对不上时，证明`item`的空间已经被回收。

当你的系统中存在大量较小值与较大值时，外存的效果就比较明显。将较大值的`key`存放在外存上时，将提高内存使用率，提高命中率。

### 2.2 Technical Breakdown

This section is a detail on the architecture and tradeoffs of external storage.

Goals
Primarily: keeping "cache patterns" in mind,get the most out of a storage device by minimizing the accessses to it.

* Larger values (typically 1k+) can use flash. Small values stay in RAM.
* A single read to serve a single key.
* Asynchronous batched writes to drive.
* Main hash table is authoritative: miss/delete/overwrite must not use the drive.
* Wrting new items to cache must never block on the flash device.
	1. Items are evicted from the LRU tail in RAM if extstore writer lags.
* Forgetting/evicting data from storage must not use the drive.

Assumptions

Some assumptions about access patterns have to be made in order to utilize flash as a cache medium. Wrting is expensive (destructive to the drive), and caches typically have much higher churn rage than databases. Flash also increases the latency of each key fetched, which can make caching small values on flash ineffective compared a range fetch against a high end database.

* Logger TTL's for data that hits disk (and good reuse)
* Latency overhead for small items is high compared to benefit of storing them.
* Latency overhead for larger items is better than a miss (within reason).
* A minority of data is hot (highly accessed), with a long tail of colder data.

**Architecure**

A review of the main threads and data structures in memcached:

![architecture](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/architecture.png)

THREADS

The listen thread accepts new connections, it then passes the socket to workers via writing a byte to a pipe. Each worker has its own "notifier pipe", which it receives events from via libevent polling. This mechanism is resued for extstore, allowing IO requests to pass to/from IO threads.

A client connection sticks to a worker thread for its lifetime.Own worker thread per CPU is ideal.

The LRU maintainer thread moves items inbetween sub-LRU's (hot|warm|cold|etc).It also makes decisions on when to schedule page moves, or LRU crawls.

The LRU cralwer reaps expired/invalid items, including items extstore invalidates.

Memory is divided into slab classes with fixed chunk sizes. Fixed size(1M) pages are assigned to each slab-class and divided into chunks as-needed. Over time, shifts in average item sizes or access patterns can "starve" a class.The page mover thread is able to safely reassign pages between slab classes to avoid strrvation.

Misc. threads: hash-table-expander,logger.These do not interact with extstore.

STRUCTURES

* Hash table is chain bucketed. "next" is embedded in item memory.
* LRU is doubly-linked, with net/prev embedded in item memory.
* Each slab class has its own LRU.
* Each LRU is split into HOT|WARM|COLD|TEMP.

Now, with extstore:

![add extstore architecture](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/extstore_architecture.png)

THREADS

The storage thread is similar to the LRU maintainer (they may merge later).It exmines the LRU tails for each slab class.It requests a write buffer from extstore, CRC32's the item data, then hands the write buffer back to extstore.

The compaction thread targets pages ready for compaction.It reads the page data back from storage,walks the items within,and either throws them away or rewrites into a new page.See below for more detail.

Multiple IO threads exist which issue pread/pwrite/etc calls to the underlying storage on behalf of worker, storage, and compaction threads.Each IO thread has a simple stack queue with a mutex.

There is also an "extstore maintenance thread",which simply sorts page data for other threads.It may disapper and become an API call for the LRU maintainer.

STRUCTURES

Extstore has some of its own data structures.Mostly stats counters,and page data.

Pages on disk are (default 64M) subsections of a file or device.Each page has a small structure in memory.The structure in memory is used to avoid writing to flash when objects are deleted from a page.A page can be reclaimed once fully empty,so a page can be recycled without reading or writing back to it.

Extstore has write buffers which items are written into from the storage or compaction threads.When write buffers are full, a write will fail temporarily write the buffer is send to an IO thread.Once flushed, writes are accepted into the buffer again.This is used to rate limit writes to storage.

通过上面的描述可以看出，增加`extstore`系统后，无论从线程还是从结构体方面都有比较大的改动，线程部分增加了`storage`线程、`IO`线程、`compaction`线程，结构体增加了`extstore page memory`、`extstore write buffers`等结构体。后面会有文章专门从源代码的角度来剖析这些新成员。

**How an item flows to storage**

![write item to extstore](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/writeitemtoextstore.png)

* If all memory is exhausted while writes to storage are waiting, items will be evicted from the LRU tail in memory.IE: An item in slab class 10 waiting to be written to storage, will instead be evicted.Sets to cache must not block on storage.

上面的图示描述了`item`在`extstore`系统中的整个`wirte`流转过程，从`LRU`的队尾进入`write buffer`中，然后通过`IO`线程刷出`write buffer`写入到`page bucket`中。

**How an item is read from storage**

![read item from extstore](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/readitemfromextstore.png)

* With multigets, many IO objects can be sent to the IO thread at teh same time.Typical flash devices are internally parallel, and can have similar latency for batched requests.
* Workers fill iovec's with response data while parsing each key in the request.For every item header,blank iovec's are added where the data would be.
* A temporary item is allocated from slab memory to hold the full item.
* If an item has become invlaid(page evicted, etc),the IO thread will record a miss back into the IO object before shipping back to the worker.
* On a hit,the holes are filled in the iovec with the temporary item.
* On a miss,the affected iovec entries are NULL'ed out, wiping the response.
* Temporary memory is freed at the end of the request.

Items read from storage have a chance of being recached into memory if they are read multiple times in less than a minute.This allows items which are only occasionally accessed to stay on storage.When they are recached into main memory,their flash entry is deleted.

图示描述了`item`存在于`extstore`系统后，从其中读取的过程，网络子线程`worker thread`通过`IO`线程来读取所需要的`item`数据，然后返回给客户端或者后续操作。

**The lifetime of a stroage page**

```
[/data/extstore]

OFFSET + ID + VERSION
0M		| P0 | 5
64M		| P1 | 10
128M	| P2 | 12
192M	| P3 | 6
256M	| P4 | 7
320M	| P5 | 8
384M	| P6 | 9
...		| .. | .
...		| .. | .
		+	  +
```
Storage pages are offsets into a file, with a default size of 64 megabytes per page. Future releases should allow multiple files (thus deivces) be used for the same page pool.As of this writing,only buffered IO is used.O_DIRECT and async IO's are planned.

Each page has a version assigned to it as it is used. When a page is recycled or evicted, it is given a new version number.

Item headers in memory store: [PAGE ID, OFFSET IN PAGE, VERSION NUMBER].The version number is used to validate an object is still valid when being fetched from storage.

Pages are orgainized into logical "buckets".In the extstore shim, buckets are arbitrary numbers.Memcached's higher levels give the buckets meaning

![page and bucket extstore](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/extstore_bucket.png)


Buckets are used to coalesce data into related pages.By default all items are written into bucket 0[DEFAULT].

* If an item has been rewritten during compaction(see below),it is written into [COMPACTED] which are reserved for items having survived compaction.
* If an item has remaining TTL at flush time (see ext_low_ttl option), it will be written into the [LOW TTL] bucket. This bucket is excluded from compaction.Pages are reclaimed periodically as the items expire.

[COMPACTED] pages tend to contain very long lived objects, so those pages have less fragmentation and need to be rewritten less often.

[LOW TTL] pages are never compacted, which avoids write amplification at the cost of some space over time.

In the future,[LOW TTL] may split into multiple buckets.Splitting IE: low, med, high, would allow very low pages to reclaim faster while still avoiding write amplification for higher TTL's.

本小节描述了`page`和`bucket`这两个概念，并且详细的说明了`bucket`和`page`之间的关系。`bucket`是一个逻辑概念，是一组具有相同目的的`page`组成。`page`的生命周期是有`page version`来控制，通过此来判断`page`中`item`的内容是否有效。

**How deletes, and overwrites work**

![delete extstore](https://github.com/whynotAC/analysis_memcached/blob/master/exstore/extstore_delete.png)


Pages track a few counters in memory only.The number of bytes stored, and the number of objects stored. A page has no way of retrieving what items are actually stored within it.

Deleting an object from a page reduces the object and byte count from the in-memory counters associated to that page. The same goes when page are being filled in the first place, the counters are incremented.

* When bytes and objects in a page reach zero, it is immediately reaped and placed onto the free list.
* Deletes or overwrites never cause new writes to happen to storage.

通过上述的描述，可以发现不同的操作对`extstore`中的`item`有着不同的处理方式。`DELETE`操作，仅需要删除`page`中内容大小;`REPLACE`操作不用对`page`中做任何操作。后续讲述`extstore`源代码时会有详细的表格给出各个操作是否会影响`extstore`。

**The compaction thread**

* First, find a candidate to compact.IE: page is more than 50% mepty.(tunded by `ext_max_frag`).
* Since pages are written to aligned by write buffer size, the thread reads data back from page in chunks the size of the write buffer (IE: 8M).
* The buffer is walked linearly. Since items are fully written to storage, the read buffer can be parsed as item structures. Once an item is examined, the next one is found by advancing a pointer by item size.
* When an item is examined:
	* The key hash is retrieved from storage pre-calculated, which speeds up readback.
	* An item lock is taken, and the hash table is checked for the original item.
	* If the item exists, and is still an item header which points into the same page version,it's queued to be rewritten.
	* If `ext_drop_unread` is enabled, a valid item is only rescued if it was recently accessed (specifically; if it's not in the COLD LRU).

`drop_unread` is important if a lot of data is being written through storage.This ensures objects which are getting requested, but not frequently enough to be recached into memory, aren't lost.

本小节描述了`compaction thread`的操作过程，主要用于将`page`整理合并，最终产生`free page`归还给`extstore`系统中。

**Memory threshoulds**

The slab page balancer has an algorithm specific for extstore configurations.

For reference, [a python](https://github.com/memcached/memcached/blob/master/scripts/memcached-automove-extstore) and [internal C verion](https://github.com/memcached/memcached/blob/master/slab_automove_extstore.c) of the algorithm exist.

When tuning changes to the algorithm, it's easy to disable the internal algorithm and use external scripts.

Since the extstore system is designed to send items to sotrage before they're removed from memory,it needs to manage a buffer of free space. 1% or 0.5%.

* In each slab class, 0.5% of chunks assigned to the class "should be free".
* Overall, 0.5% of (1M) memory pages should be kept in the global page pool, to be assigned as needed.

These targets allow the storage thread some time to react to bursts of traffic without having to drop memory. If free memory drops below the threshould, the storage thread works to catch up.

* Item headers use memory out of the global page pool, which ensures memory is available as items are pushed to storage.
* If the global pool gets low, pages are reclaimed from the larger slab classes.

*NOTE*:The algorithm is currently does not prevent small items from taking all memory. If items are relatively small, or a very large amount of storage space is available sompared to RAM, there will be no memory to hold other items.This will cause the flash to be hit much harder as very recent items are wirten out.



