# 网络子线程

通过[memcached网络通信模型](/network/memcached网络通信模型)中的图示可看出，网络子线程主要负责处理客户端发送过来的消息，然后将执行结果返回给客户端。网络子线程根据链接信息的状态在状态机中执行命令，直至链接关闭。

在介绍网络子线程之前，先回顾一下关于网络子线程的重要结构体。

```C
// 通信子线程对应的重要结构体
typedef struct {
	pthread_t thread_id;		// 线程ID
	struct event notify_event;	// 监听事件对应的通道
	int notify_receive_fd;		// 主线程与子线程通信读端
	int notify_send_fd;			// 主线程与子线程通信写端
	struct thread_stats stats; 	// 通信子线程状态 
	struct conn_queue *new_conn_queue;	// 新网络链接的链表
	cache_t *suffix_cache;		// suffix cache
#ifdef EXTSTORE
	cache_t *io_cache;			// IO objects;
	void *storage;				// data object for storage system.
#endif
	logger *l;					// logger buffer
	void *lru_bump_buf;			// async LRU bump buffer
} LIBEVENT_THREAD;

// 标注conn结构体是新产生还是重新分配
/* An item in the conneciton queue. */
enum conn_queue_item_modes {
	queue_new_conn,			// brand new connection
	queue_redispatch		// redispatching from side thread
};
typedef struct conn_queue_item CQ_ITEM;
struct conn_queue_item {
	int								sfd;	// 文件描述符
	enum conn_states				init_state; // 初始化状态
	int								event_flags; // 监听的事件
	int								read_buffer_size; // 读缓冲大小
	enum network_transport		transport;  // 网络协议
	enum conn_queue_item_modes	mode;		 // 新connection的状态
	conn 							*c;			// conn结构体的指针
	CQ_ITEM						*next;    // 下一个CQ_ITEM
};

/* A connection queue */
typedef struct conn_queue CQ;
struct conn_queue {
	CQ_ITEM *head;		// 链表头
	CQ_ITEM *tail;		// 链表尾
	pthread_mutex_t lock;	// 互斥量
};

/*
 * Each libevent instance has a wakeup pipe, which other threads
 * can use to signal that they've put a new connection on its queue.
 */
static LIBEVENT_THREAD *threads;

// 网络链接数组
conn **conns;
```

## 1. memcached网络子线程初始化

### 1.1 网络子线程初始化源代码

[memcached网络主线程](/network/网络主线程)文档中详细介绍了`memcached`主线程的初始化过程，本文将介绍`memcached`的网络子线程相关流程。其初始化的源代码如下所示:

```C
// 在memcached.c文件中
int main() {
	// 其他代码
	...
#ifdef EXTSTORE
	memcached_thread_init(settings.num_threads, storage);		// 网络通信子线程初始化函数
	...
#else
	memcached_thread_init(settings.num_threads, NULL);			// 网络通信子线程初始化函数
	...
#endif
	// 其他代码
	...
}

// 在thread.c文件中memcached_thread_init函数
void memcached_thread_init(int nthreads, void *arg) {
	// 其他代码
	···

	// 初始化网络子线程的结构体
	threads = calloc(nthreads, sizeof(LIBEVENT_THREAD));
	if (! threads) {
		perror("Can't allocate thread descriptors");
		exit(1);
	}

	// 网络相关设置代码
	for (i = 0; i < nthreads; i++) {
		int fds[2];
		// 创建管道，用于主线程与子线程之间通信
		if (pipe(fds)) {								
			perror("Can't create notify pipe");
			exit(1);
		}
		
		threads[i].notify_receive_fd = fds[0];			// 设置接收套接字
		threads[i].notify_send_fd = fds[1];				// 设置发送套接字
#ifdef EXTSTORE
		threads[i].storage = arg;
#endif
		// 网络相关设置
		setup_thread(&threads[i]);
		/* Reserve three fds for the libevent base, and two for the pipe */
		stats_state.reserved_fds += 5;
	}
	
	// 其他代码
	···
}

// thread.c文件中setup_thread函数
/*
 * Set up a thread's information
 */
static void setup_thread(LIBEVENT_THREAD *me) {
#if defined(LIBEVENT_VERSION_NUMBER) && LIBEVENT_VERSION_NUMBER >= 0x02000101
	struct event_config *ev_config;
	ev_config = event_config_new();
	event_config_set_flag(ev_config, EVENT_BASE_FLAG_NOLOCK);
	me->base = event_base_new_with_config(ev_config);
	event_config_free(ev_config);
#else
	me->base = event_init();
#endif

	// 其他代码
	···
	
	/* Listen for notifications from other threads */
	event_set(&me->notify_event, me->notify_receive_fd,
				EV_READ | EV_PERSIST, thread_libevent_process, me);
	event_base_set(me->base, &me->notify_event);
	
	// 其他代码
	···
} 
```
通过上述代码可知以下两点:

1. 网络主线程与网络子线程通过管道相互通信
2. 网络子线程监听`notify_receive_fd`(通道读端)，监听事件为`EV_READ|EV_PERSIST`,处理事件的函数为`thread_libevent_process`


### 1.2 网络子线程初始化图示

![网络子线程初始化](/images/网络子线程网络设置流程图.jpg)

## 2. 网络子线程处理函数
通过网络子线程的初始化过程可知，网络子线程通过`thread_libevent_process`函数处理通信管道读端段发送过来的信息。其代码定义如下:

```C
// 网络通信子线程使用的事件处理函数
/*
 * Processes on incoming "handle a new conneciton" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
	LIBEVENT_INFO *me = arg;
	CQ_ITEM *item;
	char buf[1];
	conn *c;
	unsigned int timeout_fd;
	
	// 读取发送过来的信息
	if (read(fd, buf, 1) != 1) {
		if (settings.verbose > 0)
			fprintf(stderr, "Can't read from libevent pipe\n");
		return;
	}
	
	// 处理发送过来的消息
	switch (buf[0]) {
	case 'c':
		// 新传递一个socket需要进行处理
		item = cq_pop(me->new_conn_queue); // 从线程的新链接的链表中获取一个CQ_ITEM
		
		if (NULL == item) {
			break;
		}
		switch (item->mode) {
			case queue_new_conn:
				// 处理新的网络套接字
				c = conn_new(item->sfd, item->init_state, item->event_flags,
								item->read_buffer_size, item->transport,
								me->base);
				if (c == NULL) {
					if (IS_UDP(item->transport)) {
						fprintf(stderr, "Can't listen for events on UDP socket\n");
						exit(1);
					} else {
						if (settings.verbose > 0) {
							fprintf(stderr, "Can't listen for events on fd %d\n",
										item->sfd);
						}
						close(item->sfd);
					}
				} else {
					c->thread =me;
				}
				break;
			// 将网络套接字重新加入到Reactor管理器中
			case queue_redispatch:
				conn_worker_readd(item->c);
				break;
		}
		// 释放使用的CQ_ITEM
		cqi_free(item);
		break;
	// we were told to pause and report in
	case 'p':
		// 通知main thread子线程已经初始化完毕
		register_thread_initialized();
		break;
	// a client socket timed out
	case 't':
		// 处理超时的connection，获取超时的套接字
		if (read(fd, &timeout_fd, sizeof(timeout_fd)) != sizeof(timeout_fd)) {
			if (settings.verbose > 0)
				fprintf(stderr, "Can't read timeout fd from libevent pipe\n");
			return;
		}
		// 关闭connection
		conn_close_idle(conns[timeout_fd]);
		break;
	}
}
```
由上述代码可知网络通信子线程中事件处理器仅有三种命令，其命令字符和作用如下表格所示:

|  命令  |  用途 |
| :---- | :---- |
| 'c'	| 接收到新链接 |
| 'p'   | 通信子线程的初始化完成 |
| 't'   | 链接超时处理 |

## 3. 网络子线程处理新链接过程
由[网络主线程](/network/网络主线程)文档可知，当网络主线程接收到新的网络链接的处理步骤如下:

1. 调用`dispatch_conn_new`函数，将通信套接字及监听状态等信息包装成`CQ_ITEM`结构体
2. 采用轮流方式选出网络通信子线程`T`，然后将`CQ_ITEM`结构体插入到子线程`T`的`new_conn_queue`队列中
3. 通过通信管道向网络通信子线程`T`发送`c`命令

网络主线程通过已上三步将新链接的信息转发给网络通信子线程进行处理，本小节将详细介绍网络子线程处理新链接的过程。通过第2小节事件处理函数的源代码可知，网络通信子线程接收到`c`命令后，调用`conn_new`函数。`conn_new`函数的源代码如下:

```C
conn *conn_new(const int sfd, enum conn_states init_state,
					const int event_flags,
					const int read_buffer_size, enum network_transport transport,
					struct event_base *base) {
	conn *c;	// 新的connection结构体
	
	assert(sfd >= 0 && sfd <= max_fds);	// 判断套接字是否在[0,max_fds]的范围内
	c = conns[sfd];		// 获取sfd对应的conn结构
	
	// 判断conn结构是否存在
	if (NULL == c) {
		// sfd对应的conn结构不存在
		if (!(c = (conn *)calloc(1, sizeof(conn)))) {
			STATS_LOCK();
			stats.malloc_fails++;		// 分配失败
			STATS_UNLOCK();
			fprintf(stderr, "Failed to allocate connection object\n");
			return NULL;
		}
		MEMCACHED_CONN_CREATE(c);
		
		// 设置conn的结构体
		c->rbuf = c->wbuf = 0;
		c->ilist = 0;
		c->suffixlist = 0;
		c->iov = 0;
		c->msglist = 0;
		c->hdrbuf = 0;
		// 申请空间的大小
		c->risze = read_buffer_size;
		c->wsize = DATA_BUFFER_SIZE;
		c->isize = ITEM_LIST_INITIAL;
		c->suffixsize = SUFFIX_LIST_INITIAL;
		c->iovsize = IOV_LIST_INITIAL;
		c->msgsize = MSG_LIST_INITIAL;
		c->hdrsize = 0;
		// 空间申请
		c->rbuf = (char *)malloc((size_t)c->rsize);
		c->wbuf = (char *)malloc((size_t)c->wsize);
		c->ilist = (item **)malloc(sizeof(item *) * c->isize);
		c->suffixlist = (char **)malloc(sizeof(char *) * c->suffixsize);
		c->iov = (struct iovec *)malloc(sizeof(struct iovec) * c->iovsize);
		c->msglist = (struct msghdr *)malloc(sizeof(struct msghdr) * c->msgisze);
		
		if (c->rbuf == 0 || c->wbuf == 0 || c->ilist == 0 || c->iov == 0 ||
				c->msglist == 0 || c->suffixsize == 0) {
			conn_free(c); 	// 释放conn结构体
			STATS_LOCK();
			stats.malloc_fails++;	// 分配空间失败
			STATS_UNLOCK();
			fprintf(stderr, "Failed to allocate buffers for connection\n");
			return NULL;
		}
		
		STATS_LOCK();
		stats_state.conn_structs++;	// 记录使用的conn结构体的个数
		STATS_UNLOCK();
		
		c->sfd = sfd;		// 设置conn结构体的套接字
		conns[sfd] = c;	// 将conn结构体加入到conns指向的结构体数组中
	}
	
	c->transport = transport;	// 设置网络协议
	c->protocol = settings.binding_protocol;	
	/* unix socket mode doesn't need this, so zeroed out. but why
	 * is this done for every command? presumably for UDP
	 * mode. */
	if (!settings.socketpath) {
		c->request_addr_size = sizeof(c->request_addr);
	} else {
		c->request_addr_size = 0;
	}
	
	// 获取链接的地址，用于输出
	if (transport == tcp_transport && init_state == conn_new_cmd) {
		if (getpeername(sfd, (struct sockaddr *)&c->request_addr,
							&c->request_addr_size)) {
			perror("getpeername");
			memset(&c->request_addr, 0, sizeof(c->request_addr));
		}
	}
	
	if (settings.verbose > 1) {
		if (init_state == conn_listening) {
			fprintf(stderr, "<%d server listening (%s)\n", sfd, prot_text(c->protocol));
		} else if (IS_UDP(transport)) {
			fprintf(stderr, "<%d server listening (udp)\n", sfd);
		} else if (c->protocol == negotiating_prot) {
			fprintf(stderr, "<%d new auto-negotiating client connection\n", sfd);
		} else if (c->protocol == ascii_prot) {
			fprintf(stderr, "<%d new ascii client connection.\n", sfd);
		} else if (c->protocol == binary_prot) {
			fprintf(stderr, "<%d new binary client connection.\n", sfd);
		} else {
			fprintf(stderr, "<%d new unknown (%d) client connection\n", sfd, c->protocol);
			assert(false);
		}
	}
	
	// 设置conn结构体其他成员
	c->state = init_state;
	c->rlbytes = 0;
	c->cmd = -1;
	c->rbytes = c->wbytes = 0;
	c->wcurr = c->wbuf;
	c->rcurr = c->rbuf;
	c->ritem = 0;
	c->icurr = c->ilist;
	c->suffixcurr = c->suffixlist;
	c->ileft = 0;
	c->suffixleft = 0;
	c->iovused = 0;
	c->msgcurr = 0;
	c->msgused = 0;
	c->authenticated = false;
	c->last_cmd_time = current_time;	// initialize for idle kicker
#ifdef EXTSTORE
	c->io_wraplist = NULL;
	c->io_wrapleft = 0;
#endif
	
	c->write_and_go = init_state;
	c->write_and_free = 0;
	c->item = 0;
	
	c->noreply = false;
	
	// 设置网络套接字关心的事件以及处理函数，将套接字加入到Reactor管理器中
	event_set(&c->event, sfd, event_flags, event_handler, (void *)c);
	event_base_set(base, &c->event);
	c->ev_flags = event_falgs;
	
	if (event_add(&c->event, 0) == -1) {
		perror("event_add");
		return NULL;
	}
	
	STATS_LOCK();
	stats_state.curr_conns++;
	stats.total_conns++;
	STATS_UNLOCK();
	
	MEMCACHED_CONN_ALLOCATE(c->sfd);
	
	return c;
}
```
通过`conn_new`源代码可知，将传递过来的套接字、初始化状态以及关心事件，将其包装成`conn`结构体，然后加入到对应网络通信子线程的`Reactor`管理器中，设置`event_handler`函数作为新链接的事件处理函数。

网络主线程的事件处理函数也是`event_handler`函数，但与新链接的init_states参数设置以及绑定的`Reactor`管理器不相同。二者区别如下表格所示:

| 参数  |  主线程对应的值  | 新链接对应的值 |
| :---- | :------------ | :----------- |
| `init_states` | `conn_listening` | `conn_new_cmd` |
| 绑定的`Reactor`所在线程 | 主线程的`Reactor`管理器中 | 网络通信子线程的`Reactor`管理器中 |

网络通信子线程的`Reactor`管理器会根据网络链接的状态，在通信信息处理状态机中进行信息处理。
