# 网络链接生命周期

前面文档分别介绍了主线程和网络子线程的相关源代码。由源代码可知主线程中进行监听，接收网络链接后，通过调用`dispatch_conn_new`函数，将新的`struct conn`分配到网络通信子线程的`Reactor`管理器上，从而开启了网络链接的生命周期。本文将从以下几个方面，介绍网络链接的生命周期:

1. 网络链接生命期图示
2. 网络链接管理结构体及初始化
3. 网络链接主动退出
4. 网络链接超时退出
5. 网络链接数控制

## 1. 网络链接生命周期
根据网络链接的相关代码，绘制出网络链接的生命周期如下图所示:

![网络链接生命周期](/images/connection的生命周期.png)

## 2. 网络链接管理结构体及初始化
网络链接通过`struct conn`数组来管理网络链接，其对应的全局变量和初始化过程如下:

```C
// 全局网络链接管理结构
conn **conns;

// 其对应的初始化过程如下:
// 在memcached.c文件main函数中
int main() {
	...
	conn_init();
	...
}

/*
 * Initializes the connections array. We don't actually allocate connection
 * structures until they're needed, so as to avoid wasting memory when the
 * maximum connection count is much higher than the actual number of
 * connections.
 *
 * This does end up wasting a few pointers' worth of memory for FDs that are
 * used for things other than connections, but that's worth it in exchange for
 * being able to directly index the conns array by FD.
 */
static void conn_init(void) {
    /* We're unlikely to see an FD much higher than maxconns. */
    int next_fd = dup(1);
    int headroom = 10;      /* account for extra unexpected open FDs */
    struct rlimit rl;

    max_fds = settings.maxconns + headroom + next_fd;

    /* But if possible, get the actual highest FD we can possibly ever see. */
    if (getrlimit(RLIMIT_NOFILE, &rl) == 0) {
        max_fds = rl.rlim_max;
    } else {
        fprintf(stderr, "Failed to query maximum file descriptor; "
                        "falling back to maxconns\n");
    }

    close(next_fd);

    // 申请网络链接管理空间
    if ((conns = calloc(max_fds, sizeof(conn *))) == NULL) {
        fprintf(stderr, "Failed to allocate connection structures\n");
        /* This is unrecoverable so bail out early. */
        exit(1);
    }
}

```

## 3. 网络链接主动退出
当网络通信结束时，网络子线程会调用`drive_machine`函数中的`case conn_closing`分支进行处理，其源代码如下:

```C
// drive_machine函数中
case conn_closing:
	if (IS_UDP(c->transport))
		conn_cleanup(c);
	else
		conn_close(c);
	stop = true;
	break;

// 对于UDP的套接字进行的处理conn_cleanup函数
static void conn_cleanup(conn *c) {
	assert(c != NULL);
	
	conn_release_items(c);	// 清空conn链接的item
	
	if (c->write_and_free) {		// 清空写出缓存空间
		free(c->write_and_free);
		c->write_and_free = 0;
	}
	
	if (c->sasl_conn) {				// 清空认证信息
		assert(settings.sasl);
		sasl_dispose(&c->sasl_conn);
		c->sasl_conn = NULL;
	}
	
	if (IS_UDP(c->transport)) {		// 重新设置主线程中的UDP状态
		conn_set_state(c, conn_read);
	}
}

// 对于TCP的套接字进行的处理conn_close函数
static void conn_close(conn *c) {
	assert(c != NULL);
	
	/* delete the event, the socket and the conn */
	event_del(&c->event);		// 从Reactor管理器中去除监听套接字
	
	if (settings.verbose > 1)
		fprintf(stderr, "<%d connection closed.\n", c->sfd);
	
	conn_cleanup(c); 	// 去除conn结构体中的item
	
	MEMCACHED_CONN_RELEASE(c->sfd);
	conn_set_state(c, conn_closed);	// 设置conn的状态
	close(c->sfd);		// 关闭套接字
	
	pthread_mutex_lock(&conn_lock);
	allow_new_conns = true;				// 允许接收新connection请求
	pthread_mutex_unlock(&conn_lock);
	
	STATS_LOCK();
	stats_state.curr_conns--;
	STATS_UNLOCK();
	
	return;
}
```
通过源代码可以看出，当网络链接主动退出时，对于TCP和UDP采用不同的处理方式。但基本上都是清除`conn`结构体中的`item`链表，清除发送缓冲区等操作。TCP方式中还设置了`conn`为关闭状态，从`Reactor`管理器中取消`sfd`的事件监控套接字等操作。

## 4. 网络链接的超时退出
`memcached`中主线程会启动回话超时检查线程`conn_timeout_thread`，此线程会逐一检查`conns`指向的`struct conn`数组中的每一个`conn`，根据`conn`的最近执行`cmd`的时间来判断`conn`是否出现超时情况。

线程启动源代码如下:

```C
// memcached.c文件中main函数
if (settigs.idle_timeout && start_conn_timeout_thread() == -1) {
	exit(EXIT_FAILURE);
}

// 创建检查回话超期线程
static int start_conn_timeout_thread() {
	int ret;
	
	if (settings.idle_timeout == 0)
		return -1;
	
	if ((ret = pthread_create(&conn_timeout_tid, NULL,
			conn_timeout_thread, NULL)) != 0) {
		fprintf(stderr, "Can't create idle connection timeout thread: %s\n", strerror(ret));
		return -1;
	}
	
	return 0;
}

// 回话检测线程
#define CONNS_PER_SLICE 100
#define TIMEOUT_MSG_SIZE (1 + sizeof(int))
static void *conn_timeout_thread(void *arg) {
	int i;
	conn *c;
	char buf[TIMEOUT_MSG_SIZE];
	rel_time_t oldest_last_cmd;
	int sleep_time;
	useconds_t timeslice = 1000000 / (max_fds / CONNS_PER_SLICE);
	
	while (1) {
		if (settings.verbose > 2)
			fprintf(stderr, "idle timeout thread at top of connection list\n");
		
		oldest_last_cmd = current_time;
		
		// 依次循环判断conn是否超时
		for (i = 0; i < max_fds; i++) {
			if ((i % CONNS_PER_SLICE) == 0) {
				if (settings.verbose > 2)
					fprintf(stderr, "idle timeout thread sleeping for %ulus\n",
								(unsinged int)timeslice);
				usleep(timeslice);
			}
			
			// 判断struct conn是否申请了空间
			if (!conns[i])
				continue;
			
			c = conns[i];		// 获取对应的struct conn结构体
			
			// 判断是否是TCP端口
			if (!IS_TCP(c->transport))
				continue;
			
			// 如果struct conn的状态不为conn_new_cmd或者conn_read时，则不进行超时判断
			if (c->state != conn_new_cmd && c->state != conn_read)
				continue;
			
			// 根据struct conn结构体中最后执行cmd的时间来判断是否超时
			if ((current_time - c->last_cmd_time) > settings.idle_timeout) {
				// conn超时，则向其网络通信线程发送超时命令
				buf[0] = 't';					// 命令
				memcpy(&buf[1], &i, sizeof(int)); // 超时conn对应的下标
				// 发送命令
				if (write(c->thread->notify_send_fd, buf, TIMEOUT_MSG_SIZE) != TIMEOUT_MSG_SIZE)
					perror("Failed to write timeout to notify pipe");
			} else {
				// 最近超时的时间
				if (c->last_cmd_time < oldest_last_cmd)
					oldest_last_cmd = c->last_cmd_time;
			}
		}
		
		/* This is the soonest we could have another connection time out */
		sleep_time = settings.idle_timeout - (current_time - oldest_last_cmd) + 1;
		if (sleep_time <= 0)
			sleep_time = 1;
			
		if (settings.verbose > 2)
			fprintf(stderr,
						"idle timeout thread finished pass, sleepting for %ds\n",
						sleep_time);
		usleep((useconds_t) sleep_time * 1000000);
	}
	
	return NULL;
}

// 网络子线程处理pipe通道读端的事件
/*
 * Processes an incoming "handle a new connection" item. This is called when
 * input arrives on the libevent wakeup pipe.
 */
static void thread_libevent_process(int fd, short which, void *arg) {
	LIBEVENT_THREAD *me = arg;
	CQ_ITEM *item;
	char buf[1];
	conn *c;
	unsigned int timeout_fd;
	
	// 读取命令字符
	if (read(fd, buf, 1) != 1) {
		if (settings.verbose > 0)
			fprintf(stderr, "Can't read from libevent pipe\n");
		return;
	}
	
	// 根据命令来进行执行
	switch(buf[0]) {
	// 其他命令
	
	// a client socket timed out
	case 't':
		// 读取回话超期的文件描述符
		if (read(fd, &timeout_fd, sizeof(timeout_fd)) != sizeof(timeout_fd)) {
			if (settings.verbose > 0)
				fprintf(stderr, "Can't read timeout fd from libevent pipe\n");
			return;
		}
		//
		conn_close_idle(conns[timeout_fd]);
		break;
	}
}

// memcached.c文件中的conn_close_idle函数
void conn_close_idle(conn *c) {
	if (settings.idle_timeout > 0 &&
			(current_time - c->last_cmd_time) > settings.idle_timeout) {
		if (c->state != conn_new_cmd && c->state != conn_read) {
			if (settings.verbose > 1)
				fprintf(stderr, "fd %d wants to timeout, but isn't in read state", c->sfd);
			return;
		}
		
		if (settings.verbose > 1)
			fprintf(stderr, "Closing idle fd %d\n", c->sfd);
		
		c->thread->stats.idle_kicks++;
		
		// 设置conn的状态,走主动关闭的路径
		conn_set_state(c, conn_closing);
		drive_machine(c);
	}
}
```
通过源代码可知，检查超时线程依次检查`struct conn`结构体，其根据`last_cmd_time`来判断其是否超时。当发现`conn`结构体超时，则向对应的网络通信子线程的`pipe`管道发送`t`命令。网络通信子线程调用回调函数`thread_libevent_process`来处理超时命令，回调函数将`conn`的状态置为`conn_closing`，随后`conn`走主动退出相关代码。

## 5. 网络链接数控制
当`memcached`程序中当有过多的链接时，将会拒绝新网络链接，等待网络链接数少于阀值`settings.maxconns`时，会继续接收新的链接请求。其源代码如下:

```C
static void drive_machine(conn *c) {
	// 其他代码
	
	while (!stop) {
		
		switch(c->state) {
		case conn_listening:
			// 其他代码
			
			// 判断是否超过阀值
			if (settings.maxconns_fast &&
					stats_state.curr_conns + stats_state.reserved_fds >= settings.maxconns - 1) {
				str = "ERROR Too many open connections\r\n";
				res = write(sfd, str, strlen(str));
				close(sfd);
				STATS_LOCK();
				stats.rejected_conns++;
				STATS_UNLOCK();
			} else {
				// 接收新的connection链接请求
				dispatch_conn_new(sfd, conn_new_cmd, EV_READ | EV_PERSIST, DATA_BUFFER_SIZE, c->transport);
			}
			
		···
		}
	}
}
```
当线程暂无文件描述符可供分配时，主线程将会注册定时器，用于监控可供分配的文件描述符数量。其源代码如下:

```C
// 判断是否有文件描述符可用
static void drive_machine(conn *c) {
	// 其他代码
	
	while (!stop) {
		
		switch(c->state) {
		case conn_listening:
			addrlen = sizeof(addr);
#ifdef HAVE_ACCEPT4
			if (use_accept4) {
				sfd = accept4(c->sfd, (struct sockaddr *)&addr, &addrlen, SOCK_NONBLOCK);
			} else {
				sfd = accept(c->sfd, (struct sockaddr *)&addr, &addrlen);
			}
#else
			sfd = accept(c->sfd, (struct sockaddr *)&addr, &addrlen);
#endif
			// 判断是否accept失败
			if (sfd == -1) {
				if (use_accept4 && errno = ENOSYS) {
					use_accept4 = 0;
					continue;
				}	
				perror(use_accept4 ? "accept4()" : "accept()");
				if (errno == EAGAIN || errno == EWOULDBLOCK) {
					/* these are transient, so don't log anything */
					stop = true;
				} else if (errno == EMFILE) {
					// 当没有空余的文件描述符时操作
					if (settings.verbose > 0)
						fprintf(stderr, "Too many open connections\n");
					/*******************重点函数*****************/
					accept_new_conns(false); // 不允许接收新的connection
					/*******************重点函数*****************/
					stop = true;
				} else {
					perror("accept()");
					stop = true;
				}
				break;
			}
			
		···
		}
	}
}

/*
 * Sets whether or not we accept new connections
 */
void accept_new_conns(const bool do_accept) {
	pthread_mutex_lock(&conn_lock);
	do_accept_new_conns(do_accept);
	pthread_mutex_unlock(&conn_lock);
}

/*
 * Sets whether we are listening for new connections or not.
 * do_accept: true,允许接收新connection;false,不允许接收新connection。
 */
void do_accept_new_conns(const bool do_accept) {
	conn *next;
	
	for (next = listen_conn; next; next = next->next) {
		if (do_accept) {
			update_event(next, EV_READ | EV_PERSIST); // 更新`Reactor`管理器中监听事件
			// 开启监听队列
			if (listen(next->sfd, settings.backlog) != 0) {
				perror("listen");
			}
		} else {
			update_event(next, 0);		// 不注册`Reactor`管理器中的监听事件
			// 监听队列长度为0
			if (liten(next->sfd, 0) != 0) {
				perror("listen");
			}
		}
	}
	
	if (do_accept) {
		struct timeval maxconns_exited;
		uint64_t elapsed_us;
		gettimeofday(&maxconns_exited, NULL);
		STATS_LOCK();
		elapsed_us = (maxconns_exited.tv_sec - stats.maxconns_entered.tc_sec) * 1000000 + (maxconns_exited.tv_usec - stats.maxconns_entered.tv_usec);
		stats.time_in_listen_disabled_us += elapsed_us;
		stats_state.accepting_conns = true;
		STATS_UNLOCK();
	} else {
		// 不允许接收新的请求
		STATS_LOCK();
		stats_state.accepting_conns = false;
		gettimeofday(&stats.maxconns_entered,NULL);
		stats.liten_disabled_num++;
		STATS_UNLOCK();
		allow_new_conns = false;
		// 设置定时器，检查是否可以接受新的链接
		maxconns_handler(-42, 0, 0);
	}
}

/* This reduces the latency without adding lots of extra wiring to be able to
 * notify the listener thread of when to listen again.
 * Also, the clock timer could be broken out into its own thread and we
 * can block the listener via a condition.
 */
static volatile bool allow_new_conns = true;
static struct event maxconnsevent;
static void maxconns_handler(const int fd, const short which, void *arg) {
	struct timeval t = {.tv_sec = 0, .tv_usec = 10000};
	
	if (fd == -42 || allow_new_conns == false) {
		// 判断是否可以接受新的链接
		/* reshedule in 10ms if we need to keep polling */
		evtimer_set(&maxconnsevent, maxconns_handler, 0); // 设置定时器
		event_base_set(main_base, &maxconnsevent); // 将定时器绑定到Reactor管理器中
		evtimer_add(&maxconnsevent, &t);
	} else {
		// 删除定时器
		evtimer_del(&maxconnsevent);
		accept_new_conns(true);			// 允许接收新的链接
	}
}
```
上面代码中详细描述了网络链接数控制过程。当网络链接退出时，便可接收新的网络链接。

```C
// 关闭conn结构时，文件描述符空闲出来
static void conn_close(conn *c) {
	assert (c != NULL);
	
	/* delete the event, the socket and the conn */
	event_del(&c->event);
	
	if (settings.verbose > 1)
		fprintf(stderr, "<%d connection closed.\n", c->sfd);
	
	conn_cleanup(c);
	
	MEMCACHED_CONN_RELEASE(c->sfd);
	conn_set_state(c, conn_closed);
	close(c->sfd);	// 关闭文件描述符
	
	pthread_mutex_lock(&conn_lock);
	allow_new_conns = true;		// 有可用得文件描述符
	pthread_mutex_unlock(&conn_lock);
	
	STATS_LOCK();
	stats_state.curr_conns--;
	STATS_UNLOCK();
	
	return;
}
```